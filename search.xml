<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Tensorflow实战 - TensorBoard计算加速]]></title>
    <url>%2F2018%2F03%2F26%2FTensorflow%E5%AE%9E%E6%88%98%20-%20Tensorflow%E8%AE%A1%E7%AE%97%E5%8A%A0%E9%80%9F%2F</url>
    <content type="text"><![CDATA[Tensorflow计算加速Tensorflow使用GPUtf.ConfigProto1234567tf.ConfigProto一般用在创建session的时候。用来对session进行参数配置with tf.Session(config = tf.ConfigProto(...),...)#tf.ConfigProto()的参数log_device_placement=True : 是否打印设备分配日志allow_soft_placement=True ： 如果你指定的设备不存在，允许TF自动分配设备tf.ConfigProto(log_device_placement=True,allow_soft_placement=True) 深度学习训练并行模式 异步模式 同步模式 多GPU并行分布式Tensorflow]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow实战 - TensorBoard可视化]]></title>
    <url>%2F2018%2F03%2F26%2FTensorflow%E5%AE%9E%E6%88%98%20-%20TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[TensorBoard简介通过Tensorflow程序运行过程中的输出的日志文件可视化Tensorflow程序的运行状态 TensorBoard日志输出123456import tensorflow as tfinput1 = tf.constant([1.0,2.0,3.0], name="input1")input2 = tf.Variable(tf.random_uniform([3]),name="input2")output = tf.add_n([input1,input2],name="add")writer = tf.summary.FileWriter("F://log",tf.get_default_graph())writer.close() TensorBoard服务启动1tensorboard --logdir=F://log]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow实战 - 循环神经网络]]></title>
    <url>%2F2018%2F03%2F22%2FTensorflow%E5%AE%9E%E6%88%98%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[循环神经网络(RNN)长短时记忆网络结构(LTSM)LSTM单元结构示意图 遗忘门 输入门 输出门 循环神经网络的变种双向循环神经网络深层循环神经网络循环神经网络的dropout循环神经网络样例应用自然语言建模数据预处理ptb_raw_data 读取PTB原始数据，并将单词转化为单词IDptb_iterator 截断并组织成batch 时序序列预测使用TFLean自定义模型tf.contrib.learn 预测正弦函数]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow实战 - 多线程输入数据处理框架]]></title>
    <url>%2F2018%2F03%2F21%2FTensorflow%E5%AE%9E%E6%88%98%20-%20%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[多线程输入数据处理框架为了避免图像预处理成为神经网络模型训练效率的瓶颈。 经典输入数据处理流程图123456op=&gt;operation: 指定原始数据的文件列表op2=&gt;operation: 创建文件列表队列op3=&gt;operation: 从文件中读取数据op4=&gt;operation: 数据预处理op5=&gt;operation: 整理成Batch作为神经网络输入op-&gt;op2-&gt;op3-&gt;op4-&gt;op5 队列与多线程队列 FIFOQueue先进先出队列 RandomShuffleQueue随机队列 多线程 tf.Coordinator协同多线程一起停止123@should_stop@request_stop@join tf.QueueRunner启动多个线程来操作同一个队列 输入文件队列 tf.train.match_filenames_once函数来获取符合一个正则表达式的所有文件 tf.train.string_input_producer函数管理文件列表123456789101112131415161718192021222324252627# 样例代码import tensorflow as tf# 使用tf.train.match_filenames_once函数获取文件列表files = tf.train.match_filenames_once("Records/data.tfrecords-*")# 通过tf.train.string_input_producer函数创建输入队列，输入队列的文件列表为tf.train.match_filenames_once函数获取的文件列表，shuffle参数设置为False来避免随机打乱读取文件的顺序。一般情况下设置为Truefilename_queue = tf.train.string_input_producer(files, shuffle=False) # 读取并解析一个样本reader = tf.TFRecordReader()_, serialized_example = reader.read(filename_queue)features = tf.parse_single_example( serialized_example, features=&#123; 'i': tf.FixedLenFeature([], tf.int64), 'j': tf.FixedLenFeature([], tf.int64), &#125;)with tf.Session() as sess:# 使用tf.train.match_filenames_once函数时需要初始化一些变量 tf.global_variables_initializer().run() print sess.run(files)# 声明tf.train.Coordinator类来协同不同线程，并启动线程 coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord)# 多次执行获取数据的操作 for i in range(6): print sess.run([features['i'], features['j']]) coord.request_stop() coord.join(threads) 组合训练数据(batching)将多个输入样例组织成一个batch可以提高模型训练的效率，所以在得到单个样例的预处理结果之后，还需要将它们组织成batch，然后再提供给神经网络的输入层 tf.train.batch tf.train.shuffle_batch1234567# [example ,label]参数给出了需要组合的元素，一般example和label分别代表训练样本和这个样本对应的正确标签。batch_size参数给出了每个batch中样例的个数。capacity给出了队列的最大容量。队列长度 = 容量，TF将暂停入队，等待元素出队。元素个数 &lt; 容量，TF将重新启动入队example_batch, label_batch = tf.train.batch([example , label],batch_size = batch_size , capacity = capacity)# min_after_dequeue参数限制了出队时队列中元素的最少个数。当队列中元素太少时，随机打乱样例顺序的作用就不大了example_batch, label_batch = tf.train.shuffle_batch([example , label],batch_size = batch_size , capacity = capacity , min_after_dequeue = 30)# 并行化处理输入数据的方法指定num_thread参数 &gt; 1，多个线程会同时读取一个文件中的不同样例并进行预处理。多线程处理不同文件，使用tf.train.shuffle_batch_join函数 输入数据处理框架]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow实战 - 图像数据处理]]></title>
    <url>%2F2018%2F03%2F20%2FTensorflow%E5%AE%9E%E6%88%98%20-%20%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[TFRecord输入数据格式TFRecord格式介绍tf.train.Example Protocol Buffer的格式存储123456789101112131415message Example &#123; Feature features = 1;&#125;;message Feature &#123; map&lt;String, Feature&gt; features = 1;&#125;;message Feature &#123; oneof kind &#123; BytesList bytes_list = 1; FloatList float_list = 2; Int64List int64_list = 3; &#125;&#125;; 图像数据处理使用图像预处理的方法可以减小无关因素对图像识别模型效果的影响 Tensorflow图像处理函数图像编码处理Tensorflow处理图像时，将图像视为矩阵，然而图像在存储时并不直接记录这些矩阵中的数字，而是记录经过压缩编码后的结果。所以要将一张图片还原成一个三维矩阵，需要解码的过程。TF提供了对jpeg和png格式图像的编码/解码函数。 在读入图像时，要使用解码函数才能得到三维矩阵；在保存图像时，需要使用编码函数才能将三维矩阵转换为需要的编码格式。123456789101112131415161718# 读入图像的原始数据 image_raw_data = tf.gfile.FastGFile("F:/input.jpeg", 'rb').read() # 将图像使用jpeg的格式解码从而得到图像对应的三维矩阵，解码之后的结果为一个张量 img_data = tf.image.decode_jpeg(image_raw_data) # 将数据的类型转换为8位无符号整型 img_data = tf.image.convert_image_dtype(img_data, dtype=tf.uint8) # 将表示一张图片的三维矩阵重新按照png格式编码并存入文件中。 encoder_png_image = tf.image.encode_png(img_data) with tf.gfile.GFile("F:/output.png", 'wb') as f: f.write(encoder_png_image.eval()) # 按照jpeg格式编码，保存 encoder_jpeg_image = tf.image.encode_png(img_data) with tf.gfile.GFile("F:/output.jpeg", 'wb') as f: f.write(encoder_jpeg_image.eval()) 图像大小调整神经网络输入节点的个数是固定的，所以在将图像的像素作为输入提供给神经网络之前，需要将图像的大小统一。1234tf.image.resize_images(param1,param2,param3，method)@param1 原始图像@param2、@param3 调整后图像的大小 @method 调整图像大小的算法 tf.image.resize_images函数的method参数取值对应算法| method取值 | 图像大小调整算法 || ——– | —–: || 0 | 双线性插值法(Bilinear interpolation) || 1 | 最近邻居法(Nearest neighbor interpolation) || 2 | 双三次插值法(Bicubic interpolation) || 3 | 面积插值法(Area interpolation) | 图像的裁剪和填充12345678# 裁剪或者填充图像中间区域tf.image.resize_image_with_crop_or_pad(param1,param2,param3)@param1 原始图像@param2、@param3 调整后图像的大小 # 裁剪或者填充给定区域的图像tf.image.crop_to_bounding_box(param2 * param3 &lt; 原始图像尺寸)tf.image.pad_to_bounding_box 图像翻转1234567891011121314# 图像上下翻转flipped = tf.image.flip_up_down(img_data)# 图像左右翻转flipped = tf.image.flip_left_right(img_data)# 图像沿对角线翻转transposed = tf.image.transpose_image(img_data)# 以一定概率上下翻转flipped = tf.image.random_flip_up_down(img_data)# 以一定概率左右翻转flipped = tf.image.random_flip_left_right(img_data) 图像色彩调整1234567891011121314151617181920212223# 将图像的亮度+(-)aadjusted = tf.image.adjust_brightness(img_data, +(-)a)# 在[-max_data, max_data]的范围随机调整图像的亮度adjusted = tf.image.random_brightness(image, max_delta)# 将图像的对比度+(-)aadjusted = tf.image.adjust_contrast(img_data, +(-)a)# 在[lower, upper]的范围内随机调整图的对比度adjusted = tf.image.random_contrast(image, lower, upper)# 调整图像的色相adjusted = tf.image.adjust_hue(ima_data , a)# 在[-max_delta , max_delta]的范围内随机调整图像的色相# max_delta的取值在[0, 0.5]之间adjusted = tf.image.random_hue(image , max_delta)# 调整图像的饱和度adjusted = tf.image.adjust_saturation(img_data , a)adjusted = tf.image.random_saturation(image , lower , upper)# 图像标准化adjusted = tf.image.per_image_whitening(img_data)调整亮度均值为0，方差为1 处理标注框12345# 加入标注框tf.image.draw_bounding_boxes#随机截取图像tf.image.sample_distored_bounding_box]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow实战 - 卷积神经网络]]></title>
    <url>%2F2018%2F03%2F16%2FTensorflow%E5%AE%9E%E6%88%98%20-%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[经典卷积神经网络模型LeNet5模型 第一层，卷积层 第二层，池化层 第三层，卷积层 第四层，池化层 第五层，全连接层 第六层，全连接层 第七层，全连接层 1输入层 -&gt; (卷积层 +-&gt; 池化层？) +-&gt; 全连接层 + Inception-v3模型Inception结构将不同的卷积层通过并联的方式结合到一起 Inception-v3模型TensorFlow-Slim工具12net = slim.conv2d(输入节点矩阵, 当前卷积层过滤器的深度, 过滤器的尺寸)可选参数: 过滤器移动步长（stride）、是否使用全0填充（padding="SAME"/"VALID"）、激活函数选择、变量的命名空间 计算 矩阵大小的计算 参数 连接数 迁移学习将一个问题上训练好的模型通过简单的调整使其适用于一个新的问题。通过迁移学习，可以使用少量的训练数据在短时间内训练出效果还不错的神经网络。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow实战]]></title>
    <url>%2F2018%2F03%2F16%2FTensorflow%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[卷积神经网络卷积神经网络的结构 输入层 卷积层 池化层 全连接层 Softmax层 卷积层 过滤器 内核 池化层池化层使用的过滤器只影响一个深度上的节点 最大池化层 1pool 平均池化层 其他]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello,Hexo]]></title>
    <url>%2F2018%2F03%2F16%2FHello-Hexo%2F</url>
    <content type="text"></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>npm</tag>
        <tag>基础</tag>
      </tags>
  </entry>
</search>
